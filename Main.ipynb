{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from binance.client import Client\n",
    "import datetime as dt\n",
    "from pytz import timezone\n",
    "from scipy.stats import zscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Retrieval\n",
    "\n",
    "# Objective: Retrieve historical data using the API of a crypto exchange, such as Kraken or Binance. Requirements:\n",
    "# • Select either BTCUSD or ETHUSD on Kraken, or BTCUSDT or ETHUSD on Binance as the trading pair. ☑️\n",
    "# • Retrieve at least 30 days of 1-minute interval data for OHLC (Open, High, Low, Close) prices and trading volume. ☑️\n",
    "# • Provide code that demonstrates the data retrieval process via the API. ☑️\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "api_key = 'hbJL4wO7LeYX6FkKuVW08PpoBgiOUOwdUYAhYLSG40xzpaywcEPgRjPgGBXGyS3B'\n",
    "api_secret = 'qHm85rDfxljX5FhvjoAVEPbQ119Teu78SAw9KERQ3YtLD8mMHH2NRVUNef9Tb2zX'\n",
    "\n",
    "client = Client(api_key, api_secret)\n",
    "\n",
    "\n",
    "def get_historical_klines(symbol, interval, start_str, end_str=None):\n",
    "    \n",
    "    # Fetch historical klines from Binance.\n",
    "\n",
    "    # param symbol: str e.g., 'BTCUSDT'\n",
    "    # param interval: str e.g., Client.KLINE_INTERVAL_1HOUR\n",
    "    # param start_str: str e.g., '7 Nov, 2024'\n",
    "    # param end_str: str e.g., '6 Dec, 2024' (optional)\n",
    "    # possible columns:  'open_time', 'open', 'high', 'low', 'close', 'volume', 'close_time', 'quote_asset_volume', 'number_of_trades','taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'ignore'\n",
    "    # return: pd.DataFrame\n",
    "   \n",
    "    klines = client.get_historical_klines(symbol, interval, start_str, end_str)\n",
    "    df = pd.DataFrame(klines, columns=[\n",
    "        'open_time', 'open', 'high', 'low', 'close', 'volume',\n",
    "        'close_time', 'quote_asset_volume', 'number_of_trades',\n",
    "        'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'ignore'\n",
    "    ])\n",
    "    df['open_time'] = pd.to_datetime(df['open_time'], unit='ms')\n",
    "    df.set_index('open_time', inplace=True)\n",
    "    # Keep only the Open, High, Low, Close columns and convert them to float\n",
    "    df = df[['open', 'high', 'low', 'close']].astype(float)\n",
    "    return df\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#Run the program:\n",
    "\n",
    "symbol = 'BTCUSDT'\n",
    "interval = Client.KLINE_INTERVAL_1MINUTE  # 1-minute intervals\n",
    "start_date = '7 Nov, 2024'\n",
    "end_date = '6 Dec, 2024'\n",
    "\n",
    "df = get_historical_klines(symbol, interval, start_date, end_date)\n",
    "\n",
    "#save the dataframe in a csv format:\n",
    "\n",
    "file_name = \"Binance_BTCUSDT_30Days_1Min.csv\" \n",
    "df.to_csv(file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp Intervals in KRAKEN_ETHUSD_interpolated.csv:\n",
      "time\n",
      "0 days 00:01:00    3384\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Timestamps are consistent (1-minute intervals). ✔️\n",
      "\n",
      "Missing Values in KRAKEN_ETHUSD_interpolated.csv:\n",
      "time      0\n",
      "open      0\n",
      "high      0\n",
      "low       0\n",
      "close     0\n",
      "Volume    0\n",
      "dtype: int64\n",
      "No missing values in the dataset. ✔️\n",
      "\n",
      "Number of duplicate rows in KRAKEN_ETHUSD_interpolated.csv: 0\n",
      "No duplicate rows found. ✔️\n",
      "Cleaned dataset saved as KRAKEN_ETHUSD_CLEANED.csv.\n"
     ]
    }
   ],
   "source": [
    "# 2. Data Cleaning 1/3 - General Inspection of the datasets\n",
    "\n",
    "# Objective: Preprocess the raw data to make it suitable for analysis. Requirements:\n",
    "# • Handle any missing timestamps or NaN values. ☑️\n",
    "# • Remove duplicate rows, if any. ☑️\n",
    "# • Convert timestamps to the CET timezone. ☑️\n",
    "\n",
    "\n",
    "# Comment from Tim: \n",
    "# As my csv file from Task 1 already contains an ISO time, \n",
    "# I will use Matteo's csv files to demonstrate how I translate the time format from UNIX timestamp to ISO time.\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def process_and_validate_file(filename):\n",
    "\n",
    "\n",
    "    # Load the dataset\n",
    "    data = pd.read_csv(filename)\n",
    "    \n",
    "    # Convert the 'time' column to a datetime object in UTC and then to CET\n",
    "    data['time'] = pd.to_datetime(data['time'], unit='s').dt.tz_localize('UTC').dt.tz_convert('Europe/Berlin')\n",
    "    \n",
    "    # Check for missing timestamps\n",
    "    time_diff = data['time'].diff().value_counts()\n",
    "    print(f\"Timestamp Intervals in {filename}:\")\n",
    "    print(time_diff)\n",
    "    \n",
    "    if time_diff.shape[0] == 1 and time_diff.index[0] == pd.Timedelta(minutes=1):\n",
    "        print(\"\\nTimestamps are consistent (1-minute intervals). ✔️\")\n",
    "    else:\n",
    "        print(\"\\nTimestamps are not consistent. Please investigate further. ❌\")\n",
    "    \n",
    "    # Check for missing values (NaN) in the dataset\n",
    "    missing_values = data.isna().sum()\n",
    "    print(f\"\\nMissing Values in {filename}:\")\n",
    "    print(missing_values)\n",
    "    \n",
    "    if missing_values.sum() == 0:\n",
    "        print(\"No missing values in the dataset. ✔️\")\n",
    "    else:\n",
    "        print(\"There are missing values in the dataset. Please address them. ❌\")\n",
    "    \n",
    "    # Check for duplicate rows\n",
    "    duplicate_count = data.duplicated().sum()\n",
    "    print(f\"\\nNumber of duplicate rows in {filename}: {duplicate_count}\")\n",
    "    \n",
    "    if duplicate_count > 0:\n",
    "        print(\"Duplicate rows found. Consider removing them for a clean dataset. ❌\")\n",
    "    else:\n",
    "        print(\"No duplicate rows found. ✔️\")\n",
    "    \n",
    "    # Optional: Drop duplicates (uncomment if needed)\n",
    "    # data = data.drop_duplicates()\n",
    "    \n",
    "    return data\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Enter file name and run the program:\n",
    "filename = 'KRAKEN_ETHUSD_interpolated.csv'  \n",
    "processed_data = process_and_validate_file(filename)\n",
    "\n",
    "# Save the processed data to a new file\n",
    "# output_filename = \"KRAKEN_ETHUSD_CLEANED.csv\"\n",
    "# processed_data.to_csv(output_filename, index=False)\n",
    "# print(f\"Cleaned dataset saved as {output_filename}.\")\n",
    "\n",
    "# Comment from Tim: \n",
    "# If there should be missing timestamps or NaN values, one would need to discuss whether to entirely drop these rows (just like the duplicates),\n",
    "# or whether one fills it with a value, such as: Forward-fill, Backward-fill, linear interpolation, etc. It depends on the goal of the data analysis.\n",
    "# Examples for handling irregularities:\n",
    "    # Forward-fill: data.fillna(method='ffill', inplace=True)\n",
    "    # Backward-fill: data.fillna(method='bfill', inplace=True)\n",
    "    # Fill with specific value (e.g., 0): data.fillna(0, inplace=True)\n",
    "    # Drop Rows: data.dropna(inplace=True)\n",
    "    # Linear Interpolation: data.interpolate(method='linear', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing rows have been interpolated. The dataset now has 1-minute intervals.\n"
     ]
    }
   ],
   "source": [
    "# 2. Data Cleaning - Clean the Kraken datasets\n",
    "\n",
    "# Load your dataset\n",
    "file_path = 'KRAKEN_ETHUSD.csv'  # Adjust the path as needed\n",
    "eth_data = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure the 'time' column is numeric\n",
    "eth_data['time'] = pd.to_numeric(eth_data['time'], errors='coerce')\n",
    "\n",
    "# Set the time column as the index and create a complete time range\n",
    "eth_data.set_index('time', inplace=True)\n",
    "complete_time_index = pd.RangeIndex(start=eth_data.index.min(), stop=eth_data.index.max() + 60, step=60)\n",
    "\n",
    "# Reindex the dataframe to include all timestamps\n",
    "eth_data = eth_data.reindex(complete_time_index)\n",
    "\n",
    "# Reset index back to the 'time' column\n",
    "eth_data.reset_index(inplace=True)\n",
    "eth_data.rename(columns={'index': 'time'}, inplace=True)\n",
    "\n",
    "# Perform linear interpolation for missing values\n",
    "eth_data.interpolate(method='linear', inplace=True)\n",
    "\n",
    "# Save the corrected dataset (optional)\n",
    "eth_data.to_csv('KRAKEN_ETHUSD_interpolated.csv', index=False)\n",
    "\n",
    "print(\"Missing rows have been interpolated. The dataset now has 1-minute intervals.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Data Cleaning 2/3\n",
    "# • Address outliers in price or volume (explain your approach).\n",
    "\n",
    "# Comment from Tim: \n",
    "# I will use the Z-Score to identify outliers.\n",
    "# Datapoints with Z-Score beyond ±3 I will consider as an outlier.\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Data Cleaning 3/3\n",
    "# • Aggregate the data into hourly intervals while maintaining the OHLC structure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
